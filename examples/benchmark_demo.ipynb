{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "525d6500",
   "metadata": {},
   "source": [
    "# UQLM Benchmarking Framework Demo\n",
    "\n",
    "This notebook demonstrates the new benchmarking capabilities:\n",
    "- **BenchmarkRunner**: Run benchmarks with automatic caching\n",
    "- **BenchmarkAnalyzer**: Analyze and visualize results\n",
    "- **FactScoreBenchmark**: Example benchmark implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf39c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from uqlm.benchmarks import BenchmarkRunner, BenchmarkAnalyzer, FactScoreBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f990c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLMs\n",
    "test_llm = ChatVertexAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "bench_llm = ChatVertexAI(model=\"gemini-2.5-pro\")\n",
    "\n",
    "# Initialize benchmark runner\n",
    "runner = BenchmarkRunner(storage_path=\"~/.uqlm/benchmark_results\")\n",
    "\n",
    "# Initialize benchmark implementation\n",
    "fs_benchmark = FactScoreBenchmark(judge_llm=bench_llm)\n",
    "\n",
    "# Run benchmark with automatic caching\n",
    "results = await runner.run_benchmark(\n",
    "    benchmark_name=\"factscore\",\n",
    "    benchmark_implementation=fs_benchmark,\n",
    "    llm_names=[\"gemini-2.5-flash\"],\n",
    "    scorer_names=[\"LongFormUQ\"],\n",
    "    dataset_name=\"dskar/FActScore\",\n",
    "    sampling_temperature=0.4,\n",
    "    num_responses=5,\n",
    "    use_cache=True,  # Check for cached results\n",
    "    save_results=True,  # Save results to database\n",
    ")\n",
    "\n",
    "print(f\"Run ID: {results.metadata.run_id}\")\n",
    "print(f\"Status: {results.metadata.status}\")\n",
    "print(f\"Number of results: {len(results.results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = BenchmarkAnalyzer(storage_path=\"~/.uqlm/benchmark_results\")\n",
    "\n",
    "# List recent benchmark runs\n",
    "recent_runs = analyzer.list_runs(benchmark_name=\"factscore\", limit=10)\n",
    "print(\"Recent runs:\")\n",
    "print(recent_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a95d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LLM performance\n",
    "comparison = analyzer.compare_llms(benchmark_name=\"factscore\", scorer_names=[\"LongFormUQ\"])\n",
    "print(\"\\nLLM Comparison:\")\n",
    "print(comparison)\n",
    "\n",
    "# Get aggregate metrics for specific run\n",
    "aggregates = analyzer.aggregate_metrics(run_id=results.metadata.run_id, groupby=\"llm\")\n",
    "print(\"\\nAggregate Metrics:\")\n",
    "print(aggregates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results (requires matplotlib)\n",
    "if not comparison.empty:\n",
    "    analyzer.plot_results(comparison, plot_type=\"bar\")\n",
    "\n",
    "# Export comprehensive report\n",
    "analyzer.export_report(run_ids=[results.metadata.run_id], output_path=\"./factscore_report.html\", format=\"html\")\n",
    "print(\"\\nReport exported to factscore_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40087c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
