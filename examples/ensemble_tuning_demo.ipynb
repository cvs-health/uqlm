{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Tunable Ensemble for LLM Uncertainty (Advanced)\n",
    "\n",
    "<div style=\"background-color: rgba(200, 200, 200, 0.1); padding: 20px; border-radius: 8px; margin-bottom: 20px; border: 1px solid rgba(127, 127, 127, 0.2); max-width: 97.5%; overflow-wrap: break-word;\">\n",
    "  <p style=\"font-size: 16px; line-height: 1.6\">\n",
    "Ensemble UQ methods combine multiple individual scorers to provide a more robust uncertainty estimate. They offer high flexibility and customizability, allowing you to tailor the ensemble to specific use cases. This ensemble can leverage any combination of black-box, white-box, or LLM-as-a-Judge scorers offered by <code>uqlm</code>. Below is a list of the available scorers:\n",
    "\n",
    "#### Black-Box (Consistency) Scorers\n",
    "*   Non-Contradiction Probability ([Chen & Mueller, 2023](https://arxiv.org/abs/2308.16175); [Lin et al., 2025](https://arxiv.org/abs/2305.19187); [Manakul et al., 2023](https://arxiv.org/abs/2303.08896))\n",
    "*   Semantic Negentropy (based on [Farquhar et al., 2024](https://www.nature.com/articles/s41586-024-07421-0); [Kuhn et al., 2023](https://arxiv.org/pdf/2302.09664))\n",
    "*   Exact Match ([Cole et al., 2023](https://arxiv.org/abs/2305.14613); [Chen & Mueller, 2023](https://arxiv.org/abs/2308.16175))\n",
    "*   BERT-score ([Manakul et al., 2023](https://arxiv.org/abs/2303.08896); [Zheng et al., 2020](https://arxiv.org/abs/1904.09675))\n",
    "*   BLUERT ([Sellam et al., 2020](https://arxiv.org/abs/2004.04696))\n",
    "*   Normalized Cosine Similarity ([Shorinwa et al., 2024](https://arxiv.org/pdf/2412.05563); [HuggingFace](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2))\n",
    "\n",
    "#### White-Box (Token-Probability-Based) Scorers\n",
    "*   Minimum token probability ([Manakul et al., 2023](https://arxiv.org/abs/2303.08896))\n",
    "*   Length-Normalized Joint Token Probability ([Malinin & Gales, 2021](https://arxiv.org/pdf/2002.07650))\n",
    "\n",
    "#### LLM-as-a-Judge Scorers\n",
    "*   Categorical LLM-as-a-Judge ([Manakul et al., 2023](https://arxiv.org/abs/2303.08896); [Chen & Mueller, 2023](https://arxiv.org/abs/2308.16175); [Luo et al., 2023](https://arxiv.org/pdf/2303.15621))\n",
    "*   Continuous LLM-as-a-Judge ([Xiong et al., 2024](https://arxiv.org/pdf/2306.13063))\n",
    "</p>\n",
    "</div>\n",
    "    \n",
    "## üìä What You'll Do in This Demo\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 15px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>1</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section1>Set up LLM and prompts.</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Set up LLM instance and load example data prompts.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 15px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>2</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section2>Tune Ensemble Weights</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Tune the ensemble weights on a set of tuning prompts. You will execute a single <code>UQEnsemble.tune()</code> method that will generate responses, compute confidence scores, and optimize weights using a provided answer key corresponding to the provided questions.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 15px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>3</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section3>Generate LLM Responses and Confidence Scores with Tuned Ensemble.</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Generate and score LLM responses to the example questions using the tuned <code>UQEnsemble()</code> object.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 25px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>4</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section4>Evaluate Hallucination Detection Performance.</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Visualize LLM accuracy at different thresholds of the ensemble score that combines various scorers. Compute precision, recall, and F1-score of hallucination detection.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "## ‚öñÔ∏è Advantages & Limitations\n",
    "\n",
    "<div style=\"display: flex; gap: 20px\">\n",
    "  <div style=\"flex: 1; background-color: rgba(0, 200, 0, 0.1); padding: 15px; border-radius: 8px; border: 1px solid rgba(0, 200, 0, 0.2)\">\n",
    "    <h3 style=\"color: #2e8b57; margin-top: 0\">Pros</h3>\n",
    "    <ul style=\"margin-bottom: 0\">\n",
    "      <li><strong>Highly Flexible:</strong> Versatile and adaptable to various tasks and question types.</li>\n",
    "      <li><strong>Highly Customizable:</strong> Ensemble weights can be tuned for optimal performance on a specific use case.</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"flex: 1; background-color: rgba(200, 0, 0, 0.1); padding: 15px; border-radius: 8px; border: 1px solid rgba(200, 0, 0, 0.2)\">\n",
    "    <h3 style=\"color: #b22222; margin-top: 0\">Cons</h3>\n",
    "    <ul style=\"margin-bottom: 0\">\n",
    "      <li><strong>Requires More Setup:</strong> Not quite \"off-the-shelf\"; requires some effort to configure and tune the ensemble.</li>\n",
    "      <li><strong>Best for Advanced Users:</strong> Optimizing the ensemble requires a deeper understanding of the individual scorers.</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from uqlm import UQEnsemble\n",
    "from uqlm.utils import load_example_dataset, math_postprocessor, plot_model_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Set up LLM and Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will illustrate this approach using a set of math questions from the [GSM8K benchmark](https://github.com/openai/grade-school-math). To implement with your use case, simply **replace the example prompts with your data**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset - gsm8k...\n",
      "Processing dataset...\n",
      "Dataset ready!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natalia sold clips to 48 of her friends in Apr...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Betty is saving money for a new wallet which c...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Julie is reading a 120-page book. Yesterday, s...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>James writes a 3-page letter to 2 different fr...</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer\n",
       "0  Natalia sold clips to 48 of her friends in Apr...     72\n",
       "1  Weng earns $12 an hour for babysitting. Yester...     10\n",
       "2  Betty is saving money for a new wallet which c...      5\n",
       "3  Julie is reading a 120-page book. Yesterday, s...     42\n",
       "4  James writes a 3-page letter to 2 different fr...    624"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load example dataset (GSM8K)\n",
    "gsm8k = load_example_dataset(\"gsm8k\", n=100)\n",
    "gsm8k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gsm8k_tune = gsm8k.iloc[0:50]\n",
    "gsm8k_test = gsm8k.iloc[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "MATH_INSTRUCTION = \"When you solve this math problem only return the answer with no additional text.\\n\"\n",
    "tune_prompts = [MATH_INSTRUCTION + prompt for prompt in gsm8k_tune.question]\n",
    "test_prompts = [MATH_INSTRUCTION + prompt for prompt in gsm8k_test.question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use `ChatVertexAI` and `AzureChatOpenAI` to instantiate our LLMs, but any [LangChain Chat Model](https://js.langchain.com/docs/integrations/chat/) may be used. Be sure to **replace with your LLM of choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install langchain-openai\n",
    "\n",
    "# # User to populate .env file with API credentials. In this step, replace with your LLM of choice.\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "gpt = AzureChatOpenAI(\n",
    "    deployment_name=os.getenv(\"AZURECHATOPENAI_DEPLOYMENT_NAME\"),\n",
    "    openai_api_key=os.getenv(\"AZURECHATOPENAI_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURECHATOPENAI_AZURE_ENDPOINT\"),\n",
    "    openai_api_type=os.getenv(\"AZURECHATOPENAI_OPENAI_API_TYPE\"),\n",
    "    openai_api_version=os.getenv(\"AZURECHATOPENAI_OPENAI_API_VERSION\"),\n",
    "    temperature=0.9,  # User to set temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install langchain-google-vertexai\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "gemini = ChatVertexAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Tune Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `UQEnsemble()` - Ensemble of uncertainty scorers\n",
    "\n",
    "#### üìã Class Attributes\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "  <tr>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 20%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Parameter</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 25%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Type & Default</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 55%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">llm</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BaseChatModel<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">A langchain llm `BaseChatModel`. User is responsible for specifying temperature and other relevant parameters to the constructor of the provided `llm` object.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">scorers</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">List<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies which black-box, white-box, or LLM-as-a-Judge scorers to include in the ensemble. List containing instances of BaseChatModel, LLMJudge, black-box scorer names from ['semantic_negentropy', 'noncontradiction','exact_match', 'bert_score', 'bleurt', 'cosine_sim'], or white-box scorer names from [\"normalized_probability\", \"min_probability\"]. If None, defaults to the off-the-shelf BS Detector ensemble by <a href=\"https://arxiv.org/abs/2308.16175\">Chen & Mueller, 2023</a> which uses components [\"noncontradiction\", \"exact_match\",\"self_reflection\"] with respective weights of [0.56, 0.14, 0.3].</td>\n",
    "  </tr>   \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">device</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str or torch.device<br><code>default=\"cpu\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies the device that NLI model use for prediction. Only applies to 'semantic_negentropy', 'noncontradiction' scorers. Pass a torch.device to leverage GPU.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">use_best</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">bool<br><code>default=True</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies whether to swap the original response for the uncertainty-minimized response among all sampled responses based on semantic entropy clusters. Only used if `scorers` includes 'semantic_negentropy' or 'noncontradiction'.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">system_prompt</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str or None<br><code>default=\"You are a helpful assistant.\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Optional argument for user to provide custom system prompt for the LLM.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">max_calls_per_min</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">int<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies how many API calls to make per minute to avoid rate limit errors. By default, no limit is specified.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">use_n_param</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">bool<br><code>default=False</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies whether to use <code>n</code> parameter for <code>BaseChatModel</code>. Not compatible with all <code>BaseChatModel</code> classes. If used, it speeds up the generation process substantially when <code>num_responses</code> is large.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">postprocessor</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">callable<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">A user-defined function that takes a string input and returns a string. Used for postprocessing outputs.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">sampling_temperature</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">float<br><code>default=1</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">The 'temperature' parameter for LLM model to generate sampled LLM responses. Must be greater than 0.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">weights</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">list of floats<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies weight for each component in ensemble. If None, and <code>scorers</code> is not None, and defaults to equal weights for each scorer. These weights get updated with <code>tune</code> method is executed.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">nli_model_name</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str<br><code>default=\"microsoft/deberta-large-mnli\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies which NLI model to use. Must be acceptable input to <code>AutoTokenizer.from_pretrained()</code> and <code>AutoModelForSequenceClassification.from_pretrained()</code>.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### üîç Parameter Groups\n",
    "\n",
    "<div style=\"display: flex; gap: 20px; margin-bottom: 20px\">\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(0, 100, 200, 0.1); border-radius: 5px; border: 1px solid rgba(0, 100, 200, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üß† LLM-Specific</p>\n",
    "    <ul>\n",
    "      <li><code>llm</code></li>\n",
    "      <li><code>system_prompt</code></li>\n",
    "      <li><code>sampling_temperature</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(0, 200, 0, 0.1); border-radius: 5px; border: 1px solid rgba(0, 200, 0, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üìä Confidence Scores</p>\n",
    "    <ul>\n",
    "      <li><code>scorers</code></li>\n",
    "      <li><code>weights</code></li>\n",
    "      <li><code>use_best</code></li>\n",
    "      <li><code>nli_model_name</code></li>\n",
    "      <li><code>postprocessor</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(200, 150, 0, 0.1); border-radius: 5px; border: 1px solid rgba(200, 150, 0, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üñ•Ô∏è Hardware</p>\n",
    "    <ul>\n",
    "      <li><code>device</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(200, 0, 200, 0.1); border-radius: 5px; border: 1px solid rgba(200, 0, 200, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">‚ö° Performance</p>\n",
    "    <ul>\n",
    "      <li><code>max_calls_per_min</code></li>\n",
    "      <li><code>use_n_param</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "#### üíª Usage Examples\n",
    "\n",
    "```python\n",
    "# Basic usage with default parameters\n",
    "uqe = UQEnsemble(llm=llm)\n",
    "\n",
    "# Using GPU acceleration\n",
    "uqe = UQEnsemble(llm=llm, device=torch.device(\"cuda\"))\n",
    "\n",
    "# Custom scorer list\n",
    "uqe = BlackBoxUQ(llm=llm, scorers=[\"bert_score\", \"exact_match\", llm])\n",
    "\n",
    "# High-throughput configuration with rate limiting\n",
    "uqe = UQEnsemble(llm=llm, max_calls_per_min=200, use_n_param=True) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the torch device\n",
    "if torch.cuda.is_available():  # NVIDIA GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():  # macOS\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPU\n",
    "print(f\"Using {device.type} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "scorers = [\n",
    "    \"exact_match\",  # Measures proportion of candidate responses that match original response (black-box)\n",
    "    \"noncontradiction\",  # mean non-contradiction probability between candidate responses and original response (black-box)\n",
    "    \"normalized_probability\",  # length-normalized joint token probability (white-box)\n",
    "    gemini,  # LLM-as-a-judge (self)\n",
    "    gpt,  # LLM-as-a-judge (separate LLM)\n",
    "]\n",
    "\n",
    "uqe = UQEnsemble(\n",
    "    llm=gemini,\n",
    "    device=device,\n",
    "    max_calls_per_min=175,\n",
    "    # postprocessor=math_postprocessor,\n",
    "    use_n_param=False,  # Set True if using AzureChatOpenAI for faster generation\n",
    "    scorers=scorers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### üîÑ Class Methods: Tuning\n",
    "\n",
    "![Sample Image](https://raw.githubusercontent.com/cvs-health/uqlm/develop/assets/images/uqensemble_tune.png)\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "  <tr>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 25%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Method</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 75%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Description & Parameters</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; vertical-align: top; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">UQEnsemble.tune</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "      <p>Generate responses from provided prompts, grade responses with provided grader function, and tune ensemble weights. If weights and threshold objectives match, joint optimization will happen. Otherwise, sequential optimization will happen. If an optimization problem has fewer than three choice variables, grid search will happen.</p>\n",
    "      <p><strong>Parameters:</strong></p>\n",
    "      <ul>\n",
    "        <li><code>prompts</code> - (<strong>list of str</strong>) A list of input prompts for the model.</li>\n",
    "        <li><code>ground_truth_answers</code> - (<strong>List[str]</strong>) A list of ideal (correct) responses.</li>\n",
    "        <li><code>grader_function</code> - (<strong>callable, default=None</strong>) A user-defined function that takes a response and a ground truth 'answer' and returns a boolean indicator of whether the response is correct. If not provided, vectara's HHEM is used: https://huggingface.co/vectara/hallucination_evaluation_model</li>\n",
    "        <li><code>num_responses</code> - (<strong>int, default=5</strong>) The number of sampled responses used to compute consistency.</li>\n",
    "        <li><code>weights_objective</code> - (<strong>str, default='roc_auc'</strong>) Objective function for weight optimization. Must match thresh_objective if one of {'fbeta_score', 'accuracy_score', 'balanced_accuracy_score'}. If same as thresh_objective, joint optimization will be done.</li>\n",
    "        <li><code>thresh_objective</code> - (<strong>str, default='fbeta_score'</strong>) Objective function for threshold optimization via grid search. One of {'fbeta_score', 'accuracy_score', 'balanced_accuracy_score', 'roc_auc', 'log_loss'}.</li>\n",
    "        <li><code>thresh_bounds</code> - (<strong>tuple of floats, default=(0,1)</strong>) Bounds to search for threshold.</li>\n",
    "        <li><code>n_trials</code> - (<strong>int, default=100</strong>) Indicates how many trials to search over with optuna optimizer</li>\n",
    "        <li><code>step_size</code> - (<strong>float, default=0.01</strong>) Indicates step size in grid search, if used.</li>\n",
    "        <li><code>fscore_beta</code> - (<strong>float, default=1</strong>) Value of beta in fbeta_score.</li>\n",
    "      </ul>\n",
    "      <p><strong>Returns:</strong> <code>UQResult</code> containing data (prompts, responses, sampled responses, and confidence scores) and metadata</p>\n",
    "      <div style=\"background-color: rgba(0, 200, 0, 0.1); padding: 8px; border-radius: 3px; margin-top: 10px; border: 1px solid rgba(0, 200, 0, 0.2); margin-right: 5px; box-sizing: border-box; width: 100%;\">\n",
    "        <strong>üí° Best For:</strong> Tuning an optimized ensemble for detecting hallucinations in a specific use case.\n",
    "      </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that below, we are providing a grader function that is specific to our use case (math questions). If you are running this example notebook with your own prompts/questions, update the grader function accordingly. Note that the default grader function, `vectara/hallucination_evaluation_model`, is used if no grader function is provided and generally works well across use cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grade_response(response: str, answer: str) -> bool:\n",
    "    return math_postprocessor(response) == answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses...\n",
      "Generating candidate responses...\n",
      "Computing confidence scores...\n",
      "Generating LLMJudge scores...\n",
      "Generating LLMJudge scores...\n",
      "Grading responses with grader function...\n",
      "Optimizing weights...\n",
      "Optimizing threshold with grid search...\n"
     ]
    }
   ],
   "source": [
    "tune_results = await uqe.tune(\n",
    "    prompts=tune_prompts,  # prompts for tuning (responses will be generated from these prompts)\n",
    "    ground_truth_answers=gsm8k_tune[\"answer\"],  # correct answers to 'grade' LLM responses against\n",
    "    grader_function=grade_response,  # grader function to grade responses against provided answers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>sampled_responses</th>\n",
       "      <th>ensemble_score</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>noncontradiction</th>\n",
       "      <th>normalized_probability</th>\n",
       "      <th>judge_1</th>\n",
       "      <th>judge_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>72\\n</td>\n",
       "      <td>[72\\n, 72\\n, 72\\n, 72\\n, 72\\n]</td>\n",
       "      <td>0.999228</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>$10\\n</td>\n",
       "      <td>[$10\\n, $10\\n, $10\\n, $10\\n, $10\\n]</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>25\\n</td>\n",
       "      <td>[35\\n, 25\\n, 25\\n, 25\\n, 25\\n]</td>\n",
       "      <td>0.539786</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.803359</td>\n",
       "      <td>0.332811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>30\\n</td>\n",
       "      <td>[30\\n, 30\\n, 30\\n, 30\\n, 30\\n]</td>\n",
       "      <td>0.990998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>156\\n</td>\n",
       "      <td>[156\\n, 156\\n, 156\\n, 156\\n, 156\\n]</td>\n",
       "      <td>0.958242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928686</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt response  \\\n",
       "0  When you solve this math problem only return t...     72\\n   \n",
       "1  When you solve this math problem only return t...    $10\\n   \n",
       "2  When you solve this math problem only return t...     25\\n   \n",
       "3  When you solve this math problem only return t...     30\\n   \n",
       "4  When you solve this math problem only return t...    156\\n   \n",
       "\n",
       "                     sampled_responses  ensemble_score  exact_match  \\\n",
       "0       [72\\n, 72\\n, 72\\n, 72\\n, 72\\n]        0.999228          1.0   \n",
       "1  [$10\\n, $10\\n, $10\\n, $10\\n, $10\\n]        0.940828          1.0   \n",
       "2       [35\\n, 25\\n, 25\\n, 25\\n, 25\\n]        0.539786          0.8   \n",
       "3       [30\\n, 30\\n, 30\\n, 30\\n, 30\\n]        0.990998          1.0   \n",
       "4  [156\\n, 156\\n, 156\\n, 156\\n, 156\\n]        0.958242          1.0   \n",
       "\n",
       "   noncontradiction  normalized_probability  judge_1  judge_2  \n",
       "0          1.000000                1.000000      1.0      0.0  \n",
       "1          1.000000                0.965445      0.0      0.0  \n",
       "2          0.803359                0.332811      1.0      1.0  \n",
       "3          1.000000                0.985680      1.0      0.0  \n",
       "4          1.000000                0.928686      1.0      0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = tune_results.to_df()\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for exact_match: 0.25742720134368274\n",
      "Weight for noncontradiction: 0.128529861964413\n",
      "Weight for normalized_probability: 0.5747307678374962\n",
      "Weight for judge_1: 0.038540308857692746\n",
      "Weight for judge_2: 0.0007718599967152709\n",
      "Threshold: 0.96\n"
     ]
    }
   ],
   "source": [
    "for i, weight in enumerate(uqe.weights):\n",
    "    print(f\"Weight for {uqe.component_names[i]}: {weight}\")\n",
    "print(f\"Threshold: {uqe.thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned ensemble's config\n",
    "uqe_tuned_config_file = \"uqe_config_tuned.json\"\n",
    "uqe.save_config(uqe_tuned_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['exact_match',\n",
       "  'noncontradiction',\n",
       "  'normalized_probability',\n",
       "  'judge_1',\n",
       "  'judge_2'],\n",
       " [0.25742720134368274,\n",
       "  0.128529861964413,\n",
       "  0.5747307678374962,\n",
       "  0.038540308857692746,\n",
       "  0.0007718599967152709],\n",
       " 0.96)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tuned ensemble from the config file for future use\n",
    "loaded_ensemble = UQEnsemble.load_config(\"uqe_config_tuned.json\")\n",
    "loaded_ensemble.component_names, loaded_ensemble.weights, loaded_ensemble.thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Generate LLM Responses and Confidence Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate hallucination detection performance, we will generate responses and corresponding confidence scores on a holdout set using the tuned ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Class Methods: Generation + Scoring\n",
    "\n",
    "![Sample Image](https://raw.githubusercontent.com/cvs-health/uqlm/develop/assets/images/uqensemble_generate_score.png)\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "  <tr>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 25%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Method</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 75%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Description & Parameters</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; vertical-align: top; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">UQEnsemble.generate_and_score</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "      <p>Generate LLM responses, sampled LLM (candidate) responses, and compute confidence scores for the provided prompts.</p>\n",
    "      <p><strong>Parameters:</strong></p>\n",
    "      <ul>\n",
    "        <li><code>prompts</code> - (<strong>list of str</strong>) A list of input prompts for the model.</li>\n",
    "        <li><code>num_responses</code> - (<strong>int, default=5</strong>) The number of sampled responses used to compute consistency.</li>\n",
    "      </ul>\n",
    "      <p><strong>Returns:</strong> <code>UQResult</code> containing data (prompts, responses, sampled responses, and confidence scores) and metadata</p>\n",
    "      <div style=\"background-color: rgba(0, 200, 0, 0.1); padding: 8px; border-radius: 3px; margin-top: 10px; border: 1px solid rgba(0, 200, 0, 0.2); margin-right: 5px; box-sizing: border-box; width: 100%;\">\n",
    "        <strong>üí° Best For:</strong> Complete end-to-end uncertainty quantification when starting with prompts.\n",
    "      </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; vertical-align: top; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">UQEnsemble.score</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "      <p>Compute confidence scores on provided LLM responses. Should only be used if responses and sampled responses are already generated.</p>\n",
    "      <p><strong>Parameters:</strong></p>\n",
    "      <ul>\n",
    "        <li><code>prompts</code> - (<strong>list of str</strong>) A list of input prompts for the LLM.</li>\n",
    "        <li><code>responses</code> - (<strong>list of str</strong>) A list of LLM responses for the prompts.</li>\n",
    "        <li><code>sampled_responses</code> - (<strong>list of list of str, default=None</strong>) A list of lists of sampled LLM responses for each prompt. These will be used to compute consistency scores by comparing to the corresponding response from <code>responses</code>. Must be provided if using Black-Box scorers.</li>\n",
    "        <li><code>logprobs_results</code> - (<strong>list of logprobs_result, default=None</strong>) List of lists of dictionaries, each returned by BaseChatModel.agenerate. Must be provided if using white box scorers.</li>\n",
    "      </ul>\n",
    "      <p><strong>Returns:</strong> <code>UQResult</code> containing data (responses, sampled responses, and confidence scores) and metadata</p>\n",
    "      <div style=\"background-color: rgba(0, 200, 0, 0.1); padding: 8px; border-radius: 3px; margin-top: 10px; border: 1px solid rgba(0, 200, 0, 0.2); margin-right: 5px; box-sizing: border-box; width: 100%;\">\n",
    "        <strong>üí° Best For:</strong> Computing uncertainty scores when responses are already generated elsewhere.\n",
    "      </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses...\n",
      "Generating candidate responses...\n",
      "Computing confidence scores...\n",
      "Generating LLMJudge scores...\n",
      "Generating LLMJudge scores...\n"
     ]
    }
   ],
   "source": [
    "test_results = await uqe.generate_and_score(prompts=test_prompts, num_responses=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Evaluate Hallucination Detection Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate hallucination detection performance, we 'grade' the responses against an answer key. Again, note that the `grade_response` function is specific to our use case (math questions). **If you are using your own prompts/questions, update the grading method accordingly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>sampled_responses</th>\n",
       "      <th>ensemble_score</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>noncontradiction</th>\n",
       "      <th>normalized_probability</th>\n",
       "      <th>judge_1</th>\n",
       "      <th>judge_2</th>\n",
       "      <th>response_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>40\\n</td>\n",
       "      <td>[40\\n, 40\\n, 40\\n, 40\\n, 40\\n]</td>\n",
       "      <td>0.998520</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997425</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>136\\n</td>\n",
       "      <td>[80\\n, 136\\n, 88\\n, 136\\n, 112\\n]</td>\n",
       "      <td>0.634459</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.413526</td>\n",
       "      <td>0.765223</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>12\\n</td>\n",
       "      <td>[14\\n, 18\\n, 14\\n, 12\\n, 12\\n]</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.409499</td>\n",
       "      <td>0.386531</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>$36\\n</td>\n",
       "      <td>[$36\\n, 36\\n, $36\\n, $36\\n, $36\\n]</td>\n",
       "      <td>0.892476</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.985693</td>\n",
       "      <td>0.905696</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>13\\n</td>\n",
       "      <td>[13\\n, 13\\n, 13\\n, 13\\n, 13\\n]</td>\n",
       "      <td>0.989598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt response  \\\n",
       "0  When you solve this math problem only return t...     40\\n   \n",
       "1  When you solve this math problem only return t...    136\\n   \n",
       "2  When you solve this math problem only return t...     12\\n   \n",
       "3  When you solve this math problem only return t...    $36\\n   \n",
       "4  When you solve this math problem only return t...     13\\n   \n",
       "\n",
       "                    sampled_responses  ensemble_score  exact_match  \\\n",
       "0      [40\\n, 40\\n, 40\\n, 40\\n, 40\\n]        0.998520          1.0   \n",
       "1   [80\\n, 136\\n, 88\\n, 136\\n, 112\\n]        0.634459          0.4   \n",
       "2      [14\\n, 18\\n, 14\\n, 12\\n, 12\\n]        0.417067          0.4   \n",
       "3  [$36\\n, 36\\n, $36\\n, $36\\n, $36\\n]        0.892476          0.8   \n",
       "4      [13\\n, 13\\n, 13\\n, 13\\n, 13\\n]        0.989598          1.0   \n",
       "\n",
       "   noncontradiction  normalized_probability  judge_1  judge_2  \\\n",
       "0          1.000000                0.997425      1.0      1.0   \n",
       "1          0.413526                0.765223      1.0      0.0   \n",
       "2          0.409499                0.386531      1.0      1.0   \n",
       "3          0.985693                0.905696      1.0      1.0   \n",
       "4          1.000000                0.981901      1.0      1.0   \n",
       "\n",
       "   response_correct  \n",
       "0             False  \n",
       "1             False  \n",
       "2             False  \n",
       "3              True  \n",
       "4             False  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_df = test_results.to_df()\n",
    "test_result_df[\"response_correct\"] = [grade_response(r, a) for r, a in zip(test_result_df[\"response\"], gsm8k_test[\"answer\"])]\n",
    "test_result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline LLM accuracy: 0.28\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Baseline LLM accuracy: {np.mean(test_result_df[\"response_correct\"])}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Filtered LLM Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we explore ‚Äòfiltered accuracy‚Äô as a metric for evaluating the performance of our confidence scores. Filtered accuracy measures the change in LLM performance when responses with confidence scores below a specified threshold are excluded. By adjusting the confidence score threshold, we can observe how the accuracy of the LLM improves as less certain responses are filtered out.\n",
    "\n",
    "We will plot the filtered accuracy across various confidence score thresholds to visualize the relationship between confidence and LLM accuracy. This analysis helps in understanding the trade-off between response coverage (measured by sample size below) and LLM accuracy, providing insights into the reliability of the LLM‚Äôs outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_accuracies(scores=test_result_df.ensemble_score, correct_indicators=test_result_df.response_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Precision, Recall, F1-Score of Hallucination Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we compute the optimal threshold for binarizing confidence scores, using F1-score as the objective. Using this threshold, we compute precision, recall, and F1-score for black box scorer predictions of whether responses are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble F1-optimal threshold: 0.96\n"
     ]
    }
   ],
   "source": [
    "# extract optimal threshold\n",
    "best_threshold = uqe.thresh\n",
    "\n",
    "# Define score vector and corresponding correct indicators (i.e. ground truth)\n",
    "y_scores = test_result_df[\"ensemble_score\"]  # confidence score\n",
    "correct_indicators = (test_result_df.response_correct) * 1  # Whether responses is actually correct\n",
    "y_pred = [(s > best_threshold) * 1 for s in y_scores]  # predicts whether response is correct based on confidence score\n",
    "print(f\"Ensemble F1-optimal threshold: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble precision: 0.4\n",
      "Ensemble recall: 0.7142857142857143\n",
      "Ensemble f1-score: 0.5128205128205128\n"
     ]
    }
   ],
   "source": [
    "# evaluate precision, recall, and f1-score of semantic entropy predictions of correctness\n",
    "print(f\"Ensemble precision: {precision_score(y_true=correct_indicators, y_pred=y_pred)}\")\n",
    "print(f\"Ensemble recall: {recall_score(y_true=correct_indicators, y_pred=y_pred)}\")\n",
    "print(f\"Ensemble f1-score: {f1_score(y_true=correct_indicators, y_pred=y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scorer Definitions\n",
    "\n",
    "### Black-Box Scorers\n",
    "Black-Box UQ scorers exploit variation in LLM responses to the same prompt to measure semantic consistency. All scorers have outputs ranging from 0 to 1, with higher values indicating higher confidence. \n",
    "\n",
    "For a given prompt $x_i$, these approaches involves generating $m$ responses $\\tilde{\\mathbf{y}}_i = \\{ \\tilde{y}_{i1},...,\\tilde{y}_{im}\\}$, using a non-zero temperature, from the same prompt and comparing these responses to the original response $y_{i}$. We provide detailed descriptions of each below.\n",
    "\n",
    "#### Exact Match Rate (`exact_match`)\n",
    "Exact Match Rate (EMR) computes the proportion of candidate responses that are identical to the original response.\n",
    "$$     EMR(y_i; \\tilde{\\mathbf{y}}_i) = \\frac{1}{m} \\sum_{j=1}^m \\mathbb{I}(y_i=\\tilde{y}_{ij}). $$\n",
    "\n",
    "For more on this scorer, refer to [Cole et al., 2023](https://arxiv.org/abs/2305.14613).\n",
    "\n",
    "#### Non-Contradiction Probability (`noncontradiction`)\n",
    "Non-contradiction probability (NCP) computes the mean non-contradiction probability estimated by a natural language inference (NLI) model. This score is formally defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    NCP(y_i; \\tilde{\\mathbf{y}}_i) = \\frac{1}{m} \\sum_{j=1}^m(1 - p_j)\n",
    "\\end{equation}\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "    p_j = \\frac{\\eta(y_{i}, \\tilde{y}_{ij}) + \\eta(\\tilde{y}_{ij},y_i)}{2}.\n",
    "\\end{equation}\n",
    "\n",
    "Above, $\\eta(\\tilde{y}_{ij},y_i)$ denotes the contradiction probability estimated by the NLI model for response $y_i$ and candidate $\\tilde{y}_{ij}$. For more on this scorer, refer to [Chen & Mueller, 2023](https://arxiv.org/abs/2308.16175), [Lin et al., 2025](https://arxiv.org/abs/2305.19187), or [Manakul et al., 2023](https://arxiv.org/abs/2303.08896).\n",
    "\n",
    "#### Normalized Semantic Negentropy (`semantic_negentropy`)\n",
    "Normalized Semantic Negentropy (NSN) normalizes the standard computation of discrete semantic entropy to be increasing with higher confidence and have [0,1] support. In contrast to the EMR and NCP, semantic entropy does not distinguish between an original response and candidate responses. Instead, this approach computes a single metric value on a list of responses generated from the same prompt. Under this approach, responses are clustered using an NLI model based on mutual entailment. We consider the discrete version of SE, where the final set of clusters is defined  as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    SE(y_i; \\tilde{\\mathbf{y}}_i) = - \\sum_{C \\in \\mathcal{C}} P(C|y_i, \\tilde{\\mathbf{y}}_i)\\log P(C|y_i, \\tilde{\\mathbf{y}}_i),\n",
    "\\end{equation}\n",
    "where $P(C|y_i, \\tilde{\\mathbf{y}}_i)$ denotes the probability a randomly selected response $y \\in \\{y_i\\} \\cup \\tilde{\\mathbf{y}}_i $ belongs to cluster $C$, and $\\mathcal{C}$ denotes the full set of clusters of $\\{y_i\\} \\cup \\tilde{\\mathbf{y}}_i$.\n",
    "\n",
    "To ensure that we have a normalized confidence score with $[0,1]$ support and with higher values corresponding to higher confidence, we implement the following normalization to arrive at *Normalized Semantic Negentropy* (NSN):\n",
    "\\begin{equation}\n",
    "    NSN(y_i; \\tilde{\\mathbf{y}}_i) = 1 - \\frac{SE(y_i; \\tilde{\\mathbf{y}}_i)}{\\log m},\n",
    "\\end{equation}\n",
    "where $\\log m$ is included to normalize the support.\n",
    "\n",
    "#### BERTScore (`bert_score`)\n",
    "Let a tokenized text sequence be denoted as $\\textbf{t} = \\{t_1,...t_L\\}$ and the corresponding contextualized word embeddings as $\\textbf{E} = \\{\\textbf{e}_1,...,\\textbf{e}_L\\}$, where $L$ is the number of tokens in the text. The BERTScore precision, recall, and F1-scores between two tokenized texts  $\\textbf{t}, \\textbf{t}'$ are respectively defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    BertP(\\textbf{t}, \\textbf{t}') = \\frac{1}{| \\textbf{t}|} \\sum_{t \\in \\textbf{t}} \\max_{t' \\in \\textbf{t}'} \\textbf{e} \\cdot \\textbf{e}'\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    BertR(\\textbf{t}, \\textbf{t}') = \\frac{1}{| \\textbf{t}'|} \\sum_{t' \\in \\textbf{t}'} \\max_{t \\in \\textbf{t}} \\textbf{e} \\cdot \\textbf{e}'\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    BertF(\\textbf{t}, \\textbf{t}') = 2\\frac{ BertP(\\textbf{t}, \\textbf{t}')  BertR(\\textbf{t}, \\textbf{t}')}{BertPr(\\textbf{t}, \\textbf{t}')  + BertRec(\\textbf{t}, \\textbf{t}')},\n",
    "\\end{equation}\n",
    "where $e, e'$ respectively correspond to $t, t'$. We compute our BERTScore-based confidence scores as follows:\n",
    "\\begin{equation}\n",
    "    BertConfidence(y_i; \\tilde{\\mathbf{y}}_i) = \\frac{1}{m} \\sum_{j=1}^m BertF(y_i, \\tilde{y}_{ij}),\n",
    "\\end{equation}\n",
    "i.e. the average BERTScore F1 across pairings of the original response with all candidate responses. For more on BERTScore, refer to [Zheng et al., 2020](https://arxiv.org/abs/1904.09675).\n",
    "\n",
    "#### BLEURT (`bleurt`)\n",
    "In contrast to the aforementioned scorers, BLEURT is specifically pre-trained and fine-tuned to learn human judgments of text similarity. Our BLEURT confidence score is the average BLEURT value across pairings of the original response with all candidate responses:\n",
    "\n",
    "\\begin{equation}\n",
    "    BLEURTConfidence(y_i; \\tilde{\\mathbf{y}}_i) = \\frac{1}{m} \\sum_{j=1}^m BLEURT(y_i, \\tilde{y}_{ij}).\n",
    "\\end{equation}\n",
    "\n",
    "For more on this scorer, refer to [Sellam et al., 2020](https://arxiv.org/abs/2004.04696).\n",
    "\n",
    "\n",
    "#### Normalized Cosine Similarity (`cosine_sim`)\n",
    "This scorer leverages a sentence transformer to map LLM outputs to an embedding space and measure similarity using those sentence embeddings. Let $V: \\mathcal{Y} \\xrightarrow{} \\mathbb{R}^d$ denote the sentence transformer, where $d$ is the dimension of the embedding space. The average cosine similarity across pairings of the original response with all candidate responses is given as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    CS(y_i; \\tilde{\\mathbf{y}}_i) = \\frac{1}{m} \\sum_{i=1}^m   \\frac{\\mathbf{V}(y_i) \\cdot \\mathbf{V}(\\tilde{y}_{ij}) }{ \\lVert \\mathbf{V}(y_i) \\rVert \\lVert \\mathbf{V}(\\tilde{y}_{ij}) \\rVert}.\n",
    "\\end{equation}\n",
    "\n",
    "To ensure a standardized support of $[0, 1]$, we normalize cosine similarity to obtain confidence scores as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    NCS(y_i; \\tilde{\\mathbf{y}}_i) = \\frac{CS(y_i; \\tilde{\\mathbf{y}}_i) + 1}{2}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### White-Box UQ Scorers\n",
    "White-box UQ scorers leverage token probabilities of the LLM's generated response to quantify uncertainty. All scorers have outputs ranging from 0 to 1, with higher values indicating higher confidence. We define two white-box UQ scorers below.\n",
    "\n",
    "#### Length-Normalized Token Probability (`normalized_probability`)\n",
    "Let the tokenization LLM response $y_i$ be denoted as $\\{t_1,...,t_{L_i}\\}$, where $L_i$ denotes the number of tokens the response. Length-normalized token probability (LNTP) computes a length-normalized analog of joint token probability:\n",
    "\n",
    "\\begin{equation}\n",
    "    LNTP(y_i) = \\prod_{t \\in y_i}  p_t^{\\frac{1}{L_i}},\n",
    "\\end{equation}\n",
    "where $p_t$ denotes the token probability for token $t$. Note that this score is equivalent to the geometric mean of token probabilities for response $y_i$. For more on this scorer, refer to [Malinin & Gales, 2021](https://arxiv.org/pdf/2002.07650).\n",
    "\n",
    "\n",
    "#### Minimum Token Probability (`min_probability`)\n",
    "Minimum token probability (MTP) uses the minimum among token probabilities for a given responses as a confidence score:\n",
    "\n",
    "\\begin{equation}\n",
    "    MTP(y_i) = \\min_{t \\in y_i}  p_t,\n",
    "\\end{equation}\n",
    "where $t$ and $p_t$ follow the same definitions as above. For more on this scorer, refer to [Manakul et al., 2023](https://arxiv.org/abs/2303.08896).\n",
    "\n",
    "### LLM-as-a-Judge Scorers\n",
    "Under the LLM-as-a-Judge approach, either the same LLM that was used for generating the original responses or a different LLM is asked to form a judgment about a pre-generated response. Below, we define two LLM-as-a-Judge scorer templates. \n",
    "#### Categorical Judge Template (`true_false_uncertain`)\n",
    "We follow the approach proposed by [Chen & Mueller, 2023](https://arxiv.org/abs/2308.16175) in which an LLM is instructed to score a question-answer concatenation as either  *incorrect*, *uncertain*, or *correct* using a carefully constructed prompt. These categories are respectively mapped to numerical scores of 0, 0.5, and 1. We denote the LLM-as-a-judge scorers as $J: \\mathcal{Y} \\xrightarrow[]{} \\{0, 0.5, 1\\}$. Formally, we can write this scorer function as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "J(y_i) = \\begin{cases}\n",
    "    0 & \\text{LLM states response is incorrect} \\\\\n",
    "    0.5 & \\text{LLM states that it is uncertain} \\\\\n",
    "    1 & \\text{LLM states response is correct}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "#### Continuous Judge Template (`continuous`)\n",
    "For the continuous template, the LLM is asked to directly score a question-answer concatenation's correctness on a scale of 0 to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬© 2025 CVS Health and/or one of its affiliates. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "uqlm",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "uqlm-FhxZH16K-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
